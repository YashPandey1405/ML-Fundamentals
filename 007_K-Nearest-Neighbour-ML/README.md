# üîç K-Nearest Neighbors (KNN) in Machine Learning

This repository contains hands-on Jupyter Notebook implementations of the **K-Nearest Neighbors (KNN)** algorithm for both **Classification** and **Regression** tasks, along with **Hyperparameter Tuning** to optimize performance.

---

## üåü What is K-Nearest Neighbors (KNN)?

**K-Nearest Neighbors (KNN)** is one of the simplest and most intuitive machine learning algorithms, based on the idea that **similar data points are likely to have similar outcomes**.

### Key highlights of KNN:

- üìà **Instance-based learning**: KNN does not learn a model explicitly; it memorizes the training data.
- üß† **Lazy learning**: Computation is deferred until prediction time.
- üõ£Ô∏è **Distance metrics**: Predictions are based on calculating distances (like Euclidean distance) between points.
- üå≥ **K-D Tree** and **Ball Tree**: Efficient data structures used to speed up KNN queries on large datasets.

### Applications of KNN:

- Classification tasks (e.g., handwriting recognition)
- Regression tasks (e.g., predicting house prices)

---

## üìö Contents

The folder includes the following notebooks:

- ‚úÖ **KNN Classifier** (with Hyperparameter Tuning)
- ‚úÖ **KNN Regressor** (with Hyperparameter Tuning)

Additionally, concepts like **KNN Algorithm**, **K-D Tree**, and **Ball Tree** are explored before implementation.

---

## üë®‚Äçüíª Author

> Maintained by Yash Pandey.  
> Feel free to ‚≠ê the repository if you find it helpful and educational!
