# ðŸ“š Bagging in Machine Learning

### ðŸš€ What is Bagging?
- **Bagging** (Bootstrap Aggregating) is an ensemble technique that:
  - Trains **multiple models** on **different random subsets** of data,
  - **Combines** their predictions (by averaging or voting),
  - **Reduces variance**, **prevents overfitting**, and **improves model stability**.

- Common example: **Random Forest** (bagging applied to decision trees).

---

### ðŸ“¦ Topics Covered:

1. **Custom Bagging Classifier**  
   â†’ Built a custom bagging model for classification on randomly created data.

2. **Custom Bagging Regressor**  
   â†’ Built a custom bagging model for regression on randomly created data.

3. **Pipeline & ColumnTransformer**  
   â†’ Automated feature engineering (missing values, encoding, scaling) using Pipeline and ColumnTransformer.

4. **Random Forest Classifier on 'Tips' Dataset**  
   â†’ Applied Random Forest for classification tasks on real-world data.

5. **Random Forest Regressor on 'Tips' Dataset**  
   â†’ Applied Random Forest for regression tasks on the same real-world dataset.

6. **Out-of-Bag (OOB) Score**  
   â†’ Evaluated Random Forest models using Out-of-Bag score .

---

# ðŸŒŸ Bagging = **Reduce Variance + Improve Stability + Boost Accuracy**